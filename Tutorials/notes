# CUDA Notes
## CUDA execution flow
1. Allocate host memory and initialize host data (input data)
2. Allocate device memory
3. Transfer input data from host to device memory.
4. Execute kernels
5. Transfer output (result) from device memory to host memory.

## Concurrent Programming vs Parallel Programming
Parallel: Programs are executed simultaneously on separate hardware,independent of each other.
Concurrent: Programs seem to run simultaneously on the same/separate hardware.

## CUDA Kernel Launch
CUDA kernel launches are specified using the triple angle bracket syntax: <<< >>>.

`kernel<<<num_thread_block, num_threads_per_block, shared_memory_size>>>`

Functions on GPU called from CPU are declared using `__global__`
- Runs on the device
- Called from host code

Functions on GPU called from GPU are declared using `__device__`
- Runs on the device
- is called from device code

A kernel is launched as a grid of blocks of threads.
- `blockIdx` and `threadIdx` are 3D

`threadIdx.x`: index of the current thread within its block

`blockIdx.x`: index of the current thread block within the grid

`blockDim.x`: number of threads in a block.

`gridDim.x`: number of blocks in the grid.

`cudaDeviceSynchronize()`: tells CPU to wait until all threads in kernel are done before accessing results.


## CUDA GPUs Execution model
CUDA GPUs have many Streaming Multiprocessors, or SMs, which are groups of parallel processors. 
Each SM contains multiple parallel processors.
Each SM can run multiple concurrent thread blocks.
